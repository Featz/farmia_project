{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c72a3435-781a-48f2-822c-598b4ad30ed2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTILS: Usando SparkSession activa existente. AppName: Databricks Shell\nINFO: Cargando configuración para el entorno: databricks\nUTILS: Cargando configuración desde: /dbfs/FileStore/configs/farmia_ingest_config.json\nUTILS: Configuración cargada exitosamente.\n\nINFO: Rutas obtenidas de la configuración:\n  Landing: abfss://landing@masterfrl001sta.dfs.core.windows.net/sales_online/\n  Archive: abfss://archive@masterfrl001sta.dfs.core.windows.net/sales_online_archived/\n  Raw: abfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/\n  Bronze delta path: abfss://lakehouse@masterfrl001sta.dfs.core.windows.net/bronze/sales_online_delta/\n  Bronze table fullname: farmia_bronze.sales_online_orders\n"
     ]
    }
   ],
   "source": [
    "#Configuraciones\n",
    "# Importar la función orquestadora desde el paquete instalado\n",
    "from farmia_engine.main_orchestrators import execute_landing_to_raw, execute_raw_to_bronze, utils\n",
    "import traceback # Importar para un mejor log de errores\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Definir el entorno y el dataset que queremos consultar\n",
    "entorno_actual = \"databricks\" # Estamos en un notebook de Databricks\n",
    "dataset_name_key_to_query = \"sales_online_csv\" # El dataset que vamos a validar\n",
    "\n",
    "# Obtener la SparkSession (utils.py la configurará o reutilizará la existente)\n",
    "spark = utils.get_spark_session(env_type=entorno_actual, app_name=f\"QueryLayers_{dataset_name_key_to_query}\")\n",
    "# dbutils está disponible globalmente en notebooks Python de Databricks, pero get_dbutils() lo recupera de forma segura.\n",
    "dbutils_obj = utils.get_dbutils() \n",
    "\n",
    "paths_for_listing = {} # Diccionario para guardar las rutas que necesitamos\n",
    "\n",
    "try:\n",
    "    print(f\"INFO: Cargando configuración para el entorno: {entorno_actual}\")\n",
    "    # utils.load_app_config usará la ruta DATABRICKS_DBFS_CONFIG_PATH por defecto para Databricks\n",
    "    # (ej. /dbfs/FileStore/configs/farmia_ingest_config.json)\n",
    "    # Si tu config.json está en otra ruta en DBFS, pásala como config_path_override\n",
    "    # ej. config_general = utils.load_app_config(entorno_actual, config_path_override=\"/dbfs/mi/otra/ruta/config.json\")\n",
    "    config_general = utils.load_app_config(entorno_actual)\n",
    "\n",
    "    dataset_cfg = config_general.get(\"dataset_configs\", {}).get(dataset_name_key_to_query)\n",
    "    if not dataset_cfg:\n",
    "        raise ValueError(f\"Configuración para el dataset '{dataset_name_key_to_query}' no encontrada en config.json.\")\n",
    "\n",
    "    databricks_env_cfg_base = config_general.get(\"databricks_env_config\")\n",
    "    adls_base_cfg = config_general.get(\"adls_config_base\")\n",
    "\n",
    "    if not databricks_env_cfg_base or not adls_base_cfg:\n",
    "        raise ValueError(\"Configuraciones 'databricks_env_config' o 'adls_config_base' no encontradas en config.json.\")\n",
    "\n",
    "    # Construir todas las rutas para el dataset especificado\n",
    "    all_paths = utils.construct_full_paths_for_dataset(\n",
    "        databricks_env_cfg_base, \n",
    "        adls_base_cfg, \n",
    "        dataset_cfg, \n",
    "        dataset_name_key_to_query, \n",
    "        entorno_actual\n",
    "    )\n",
    "\n",
    "    # Extraer las rutas específicas que necesitamos para listar\n",
    "    paths_for_listing[\"landing\"] = all_paths.get(\"source_path\")\n",
    "    paths_for_listing[\"archive\"] = all_paths.get(\"archive_path\")\n",
    "    paths_for_listing[\"raw\"] = all_paths.get(\"raw_target_path\")\n",
    "    paths_for_listing[\"bronze_delta_path\"] = all_paths.get(\"bronze_target_path\")\n",
    "    \n",
    "    # Nombre de la tabla bronze para consultas SQL\n",
    "    bronze_cfg = dataset_cfg.get(\"bronze_config\", {})\n",
    "    bronze_db_name = bronze_cfg.get(\"bronze_database_name\", \"default\")\n",
    "    bronze_table_name_only = bronze_cfg.get(\"bronze_table_name\", dataset_name_key_to_query.replace(\"/\",\"_\") + \"_bronze\")\n",
    "    paths_for_listing[\"bronze_table_fullname\"] = f\"{bronze_db_name}.{bronze_table_name_only}\"\n",
    "\n",
    "\n",
    "    print(\"\\nINFO: Rutas obtenidas de la configuración:\")\n",
    "    for key, path_val in paths_for_listing.items():\n",
    "        if path_val: # Solo imprimir si la ruta fue construida\n",
    "             print(f\"  {key.replace('_', ' ').capitalize()}: {path_val}\")\n",
    "        else:\n",
    "             print(f\"  {key.replace('_', ' ').capitalize()}: No configurada o no aplicable.\")\n",
    "    \n",
    "    # Validar que las rutas esenciales para las siguientes celdas estén definidas\n",
    "    if not paths_for_listing.get(\"landing\") or \\\n",
    "       not paths_for_listing.get(\"archive\") or \\\n",
    "       not paths_for_listing.get(\"raw\") or \\\n",
    "       not paths_for_listing.get(\"bronze_delta_path\"):\n",
    "        dbutils.notebook.exit(\"ERROR: Una o más rutas esenciales no pudieron ser construidas desde la configuración.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR FATAL al cargar la configuración o definir rutas: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    dbutils.notebook.exit(f\"ERROR FATAL: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d05a76e8-3fe8-4d02-90af-78af23e1db0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Contenido de la Capa LANDING (Fuente Original) ---\nRuta a listar: abfss://landing@masterfrl001sta.dfs.core.windows.net/sales_online/\nNOTA: Después del procesamiento y archivado, esta carpeta debería estar vacía o solo con archivos nuevos.\nEl directorio de landing está vacío o no existe (esperado si los archivos fueron archivados).\n"
     ]
    }
   ],
   "source": [
    "# CELDA PARA LISTAR LANDING (FUENTE ORIGINAL)\n",
    "\n",
    "landing_path_to_list = paths_for_listing.get(\"landing\")\n",
    "\n",
    "if landing_path_to_list:\n",
    "    print(f\"--- Contenido de la Capa LANDING (Fuente Original) ---\")\n",
    "    print(f\"Ruta a listar: {landing_path_to_list}\")\n",
    "    print(\"NOTA: Después del procesamiento y archivado, esta carpeta debería estar vacía o solo con archivos nuevos.\")\n",
    "    try:\n",
    "        files = dbutils.fs.ls(landing_path_to_list)\n",
    "        if not files:\n",
    "            print(\"El directorio de landing está vacío o no existe (esperado si los archivos fueron archivados).\")\n",
    "        for f in files:\n",
    "            print(f.path)\n",
    "    except Exception as e:\n",
    "        if \"java.io.FileNotFoundException\" in str(e):\n",
    "            print(f\"El directorio de landing no existe o está vacío: {landing_path_to_list}\")\n",
    "        else:\n",
    "            print(f\"Error listando el directorio de landing: {e}\")\n",
    "else:\n",
    "    print(\"ERROR: La ruta de Landing no está definida para listar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c877ce7-a53e-461a-a14e-58087dc410d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Landing -> Raw\n",
    "\n",
    "print(\"INFO: Iniciando pipeline Landing -> Raw en Databricks...\")\n",
    "final_status_message = \"\"\n",
    "success = False\n",
    "\n",
    "try:\n",
    "    execute_landing_to_raw()\n",
    "    print(\"INFO: Pipeline Landing -> Raw completado exitosamente.\")\n",
    "    final_status_message = \"SUCCESS - Landing to Raw\"\n",
    "    success = True\n",
    "except Exception as e:\n",
    "    print(f\"ERROR durante el pipeline Landing -> Raw: {str(e)}\")\n",
    "    # Obtener el traceback completo para un diagnóstico más detallado\n",
    "    detailed_error = traceback.format_exc()\n",
    "    print(detailed_error)\n",
    "    final_status_message = f\"FAILURE - Landing to Raw: {str(e)}\\n\\nTraceback:\\n{detailed_error}\"\n",
    "    success = False\n",
    "finally:\n",
    "    if not final_status_message: # Fallback por si algo muy extraño ocurre\n",
    "        final_status_message = \"FAILURE - Landing to Raw: Estado final desconocido o ejecución interrumpida.\"\n",
    "        success = False\n",
    "    \n",
    "    print(f\"INFO: Mensaje de salida final del notebook: {final_status_message}\")\n",
    "    if success:\n",
    "        dbutils.notebook.exit(final_status_message)\n",
    "    else:\n",
    "        # Cuando se usa dbutils.notebook.exit con un string que NO comienza con \"SUCCESS\", \n",
    "        # el job se marca como fallido y el string es el mensaje de error.\n",
    "        dbutils.notebook.exit(final_status_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8008aa1-9922-4764-87d7-3c28d3aa7814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Contenido de la Capa ARCHIVE (Landing Procesado) ---\nRuta a listar: abfss://archive@masterfrl001sta.dfs.core.windows.net/sales_online_archived/\nNOTA: Aquí deberían estar los archivos CSV originales después de ser procesados por landing-to-raw.\nEl directorio de archivo está vacío o no existe.\n"
     ]
    }
   ],
   "source": [
    "# CELDA PARA LISTAR ARCHIVE (LANDING PROCESADO)\n",
    "\n",
    "archive_path_to_list = paths_for_listing.get(\"archive\")\n",
    "\n",
    "if archive_path_to_list:\n",
    "    print(f\"\\n--- Contenido de la Capa ARCHIVE (Landing Procesado) ---\")\n",
    "    print(f\"Ruta a listar: {archive_path_to_list}\")\n",
    "    print(\"NOTA: Aquí deberían estar los archivos CSV originales después de ser procesados por landing-to-raw.\")\n",
    "    try:\n",
    "        files = dbutils.fs.ls(archive_path_to_list)\n",
    "        if not files:\n",
    "            print(\"El directorio de archivo está vacío o no existe.\")\n",
    "        for f in files:\n",
    "            print(f.path)\n",
    "    except Exception as e:\n",
    "        if \"java.io.FileNotFoundException\" in str(e):\n",
    "            print(f\"El directorio de archivo no existe: {archive_path_to_list}\")\n",
    "        else:\n",
    "            print(f\"Error listando el directorio de archivo: {e}\")\n",
    "else:\n",
    "    print(\"ERROR: La ruta de Archive no está definida para listar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2183229f-fe4c-4a73-94b7-0dca9334bc91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Contenido de la Capa RAW (Parquet Particionado) ---\nRuta a listar: abfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/\nNOTA: Esperar estructura de directorios Parquet, posiblemente particionada (ej. year=...).\nListando recursivamente (RAW): abfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/\nabfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/_SUCCESS\nabfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/\nListando recursivamente (RAW): abfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/\nabfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/month=5/\nListando recursivamente (RAW): abfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/month=5/\nabfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/month=5/day=21/\nListando recursivamente (RAW): abfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/month=5/day=21/\nabfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/month=5/day=21/_SUCCESS\nabfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/month=5/day=21/_committed_3744309317735724355\nabfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/month=5/day=21/_committed_4992565752176040595\nabfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/month=5/day=21/_committed_vacuum2191691545171625321\nabfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/month=5/day=21/_started_3744309317735724355\nabfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/month=5/day=21/part-00000-tid-3744309317735724355-5984ff94-08c8-4c44-b9c5-8f6aabc31457-15-1.c000.snappy.parquet\nabfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/month=5/day=21/part-00000-tid-4992565752176040595-8ea03453-9d08-483f-b93e-df532077e2ab-142-1.c000.snappy.parquet\nabfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/month=5/day=21/part-00001-tid-4992565752176040595-8ea03453-9d08-483f-b93e-df532077e2ab-143-1.c000.snappy.parquet\nabfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/month=5/day=22/\nListando recursivamente (RAW): abfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/month=5/day=22/\nabfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/month=5/day=22/_SUCCESS\nabfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/month=5/day=22/_committed_6589603812544605794\nabfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/month=5/day=22/_started_6589603812544605794\nabfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/year=2025/month=5/day=22/part-00000-tid-6589603812544605794-2926c631-f0b8-4780-9621-8a54dec9d78c-9-1.c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# CELDA PARA LISTAR RAW (PARQUET)\n",
    "\n",
    "raw_path_to_list = paths_for_listing.get(\"raw\")\n",
    "\n",
    "def list_files_recursively_generic(path_to_list, path_label=\"RAW\"):\n",
    "    print(f\"Listando recursivamente ({path_label}): {path_to_list}\")\n",
    "    try:\n",
    "        for i in dbutils.fs.ls(path_to_list):\n",
    "            print(i.path)\n",
    "            # Evitar recursar en _delta_log u otras carpetas de metadatos si es necesario\n",
    "            if i.isDir() and not i.name.startswith(\"_\") and not i.name.startswith(\".\"): \n",
    "                list_files_recursively_generic(i.path, path_label) # Llamada recursiva\n",
    "    except Exception as e:\n",
    "        if \"java.io.FileNotFoundException\" in str(e):\n",
    "             print(f\"  El subdirectorio o archivo no existe: {path_to_list}\")\n",
    "        else:\n",
    "            print(f\"  Error listando {path_to_list}: {e}\")\n",
    "\n",
    "\n",
    "if raw_path_to_list:\n",
    "    print(f\"\\n--- Contenido de la Capa RAW (Parquet Particionado) ---\")\n",
    "    print(f\"Ruta a listar: {raw_path_to_list}\")\n",
    "    print(\"NOTA: Esperar estructura de directorios Parquet, posiblemente particionada (ej. year=...).\")\n",
    "    try:\n",
    "        top_level_files = dbutils.fs.ls(raw_path_to_list)\n",
    "        if not top_level_files:\n",
    "            print(\"El directorio raw está vacío o no existe.\")\n",
    "        else:\n",
    "            list_files_recursively_generic(raw_path_to_list, \"RAW\")\n",
    "    except Exception as e:\n",
    "        if \"java.io.FileNotFoundException\" in str(e):\n",
    "            print(f\"El directorio raw no existe: {raw_path_to_list}\")\n",
    "        else:\n",
    "            print(f\"Error listando el directorio raw: {e}\")\n",
    "else:\n",
    "    print(\"ERROR: La ruta Raw no está definida para listar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2363a443-0367-4ad0-ac1f-80dc92c0436b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Validando Capa RAW para 'sales_online_csv' ---\nLeyendo datos Parquet desde: abfss://lakehouse@masterfrl001sta.dfs.core.windows.net/raw/sales_online_raw_parquet/\n\nEsquema del DataFrame RAW:\nroot\n |-- order_id: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- quantity: integer (nullable = true)\n |-- price: double (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- order_date: timestamp (nullable = true)\n |-- promo_code: string (nullable = true)\n |-- _rescued_data: string (nullable = true)\n |-- ingestion_date: timestamp (nullable = true)\n |-- source_filename: string (nullable = true)\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- day: integer (nullable = true)\n\n\nNúmero total de registros en RAW para esta carga/dataset: 30\n\nMostrando algunas filas de la capa RAW (incluyendo metadatos y columnas de partición si existen aquí):\n+------------------------------------+-------------------+----------+----+-----+---+-----------------------+----------------------------------------------------------------------------------------------------+\n|order_id                            |order_date         |promo_code|year|month|day|ingestion_date         |source_filename                                                                                     |\n+------------------------------------+-------------------+----------+----+-----+---+-----------------------+----------------------------------------------------------------------------------------------------+\n|6a3448c6-3818-4c7d-a641-0c266b441136|2025-05-21 20:50:03|NEW20     |2025|5    |21 |2025-05-21 21:48:52.286|abfss://landing@masterfrl001sta.dfs.core.windows.net/sales_online/sales_online_20250521214759167.csv|\n|c7e055f3-19e8-4438-8968-ad306a7a7eca|2025-05-21 20:42:48|SAVE10    |2025|5    |21 |2025-05-21 21:48:52.286|abfss://landing@masterfrl001sta.dfs.core.windows.net/sales_online/sales_online_20250521214759167.csv|\n|439ed7d0-479d-4474-a8b2-0c824806aeb0|2025-05-21 21:21:11|NULL      |2025|5    |21 |2025-05-21 21:48:52.286|abfss://landing@masterfrl001sta.dfs.core.windows.net/sales_online/sales_online_20250521214759167.csv|\n|2b32fb93-2f0b-49b4-aadc-7aca64bc6950|2025-05-21 20:36:46|FREEBIE   |2025|5    |21 |2025-05-21 21:48:52.286|abfss://landing@masterfrl001sta.dfs.core.windows.net/sales_online/sales_online_20250521214759167.csv|\n|7b16d565-8482-43b8-a443-679b862d6fca|2025-05-21 21:08:25|SUMMERDEAL|2025|5    |21 |2025-05-21 21:48:52.286|abfss://landing@masterfrl001sta.dfs.core.windows.net/sales_online/sales_online_20250521214759167.csv|\n|d890ca59-62cd-410d-881e-cf42228a52ec|2025-05-21 21:18:25|SAVE10    |2025|5    |21 |2025-05-21 21:48:52.286|abfss://landing@masterfrl001sta.dfs.core.windows.net/sales_online/sales_online_20250521214759167.csv|\n|ed4fb575-246f-4013-b058-242ba2860d21|2025-05-22 20:50:22|NULL      |2025|5    |22 |2025-05-22 22:49:42.469|abfss://landing@masterfrl001sta.dfs.core.windows.net/sales_online/sales_online_20250522223838700.csv|\n|244744b8-b689-4203-b1fa-48deac994e7c|2025-05-22 21:25:58|NULL      |2025|5    |22 |2025-05-22 22:49:42.469|abfss://landing@masterfrl001sta.dfs.core.windows.net/sales_online/sales_online_20250522223838700.csv|\n|68483797-26b5-48f3-88f0-99915635b89f|2025-05-22 22:16:15|NULL      |2025|5    |22 |2025-05-22 22:49:42.469|abfss://landing@masterfrl001sta.dfs.core.windows.net/sales_online/sales_online_20250522223838700.csv|\n|09c1fa78-4706-4979-a338-6ca2158883c1|2025-05-22 20:49:54|NULL      |2025|5    |22 |2025-05-22 22:49:42.469|abfss://landing@masterfrl001sta.dfs.core.windows.net/sales_online/sales_online_20250522223838700.csv|\n+------------------------------------+-------------------+----------+----+-----+---+-----------------------+----------------------------------------------------------------------------------------------------+\nonly showing top 10 rows\n\n\nNúmero de registros en RAW con 'promo_code' no nulo: 5\n"
     ]
    }
   ],
   "source": [
    "# CELDA DE VALIDACIÓN - CAPA RAW (sales_online_raw_parquet)\n",
    "raw_sales_online_path=  paths_for_listing[\"raw\"]\n",
    "print(f\"--- Validando Capa RAW para 'sales_online_csv' ---\")\n",
    "print(f\"Leyendo datos Parquet desde: {raw_sales_online_path}\")\n",
    "\n",
    "try:\n",
    "    df_raw_sales = spark.read.parquet(raw_sales_online_path)\n",
    "    \n",
    "    print(\"\\nEsquema del DataFrame RAW:\")\n",
    "    df_raw_sales.printSchema()\n",
    "    \n",
    "    print(f\"\\nNúmero total de registros en RAW para esta carga/dataset: {df_raw_sales.count()}\")\n",
    "    \n",
    "    print(\"\\nMostrando algunas filas de la capa RAW (incluyendo metadatos y columnas de partición si existen aquí):\")\n",
    "    # Selecciona columnas clave, incluyendo las que esperas de la evolución (promo_code)\n",
    "    # y las columnas de partición de raw (year, month, day) y las de metadatos de ingesta.\n",
    "    columnas_a_mostrar_raw = []\n",
    "    if \"order_id\" in df_raw_sales.columns: columnas_a_mostrar_raw.append(\"order_id\")\n",
    "    if \"order_date\" in df_raw_sales.columns: columnas_a_mostrar_raw.append(\"order_date\")\n",
    "    if \"promo_code\" in df_raw_sales.columns: columnas_a_mostrar_raw.append(\"promo_code\") # Para verificar evolución\n",
    "    if \"year\" in df_raw_sales.columns: columnas_a_mostrar_raw.append(\"year\") # Columna de partición de raw\n",
    "    if \"month\" in df_raw_sales.columns: columnas_a_mostrar_raw.append(\"month\")# Columna de partición de raw\n",
    "    if \"day\" in df_raw_sales.columns: columnas_a_mostrar_raw.append(\"day\") # Columna de partición de raw\n",
    "    if \"ingestion_date\" in df_raw_sales.columns: columnas_a_mostrar_raw.append(\"ingestion_date\")\n",
    "    if \"source_filename\" in df_raw_sales.columns: columnas_a_mostrar_raw.append(\"source_filename\")\n",
    "        \n",
    "    if columnas_a_mostrar_raw:\n",
    "        df_raw_sales.select(*columnas_a_mostrar_raw).show(10, truncate=False)\n",
    "    else:\n",
    "        print(\"No se encontraron columnas esperadas para mostrar, mostrando todas:\")\n",
    "        df_raw_sales.show(10, truncate=False)\n",
    "\n",
    "    # Verificar si la columna 'promo_code' existe y cuántos registros la tienen no nula\n",
    "    if \"promo_code\" in df_raw_sales.columns:\n",
    "        promo_code_not_null_count = df_raw_sales.filter(col(\"promo_code\").isNotNull()).count()\n",
    "        print(f\"\\nNúmero de registros en RAW con 'promo_code' no nulo: {promo_code_not_null_count}\")\n",
    "    else:\n",
    "        print(\"\\nLa columna 'promo_code' no existe en el DataFrame RAW leído.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al leer o procesar datos de la capa RAW: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dff9194a-a2bb-49a3-be45-c8bb4723b629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS farmia_bronze.sales_online_orders;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cab62ff-8d95-455a-838a-0e9b865b1635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Raw -> Bronze\n",
    "\n",
    "print(\"INFO: Iniciando pipeline Raw -> Bronze en Databricks...\")\n",
    "final_status_message = \"\"\n",
    "success = False\n",
    "\n",
    "try:\n",
    "    execute_raw_to_bronze()\n",
    "    print(\"INFO: Pipeline Raw -> Bronze completado exitosamente.\")\n",
    "    final_status_message = \"SUCCESS - Raw to bronze\"\n",
    "    success = True\n",
    "except Exception as e:\n",
    "    print(f\"ERROR durante el pipeline Landing -> Raw: {str(e)}\")\n",
    "    # Obtener el traceback completo para un diagnóstico más detallado\n",
    "    detailed_error = traceback.format_exc()\n",
    "    print(detailed_error)\n",
    "    final_status_message = f\"FAILURE - Raw to Bronze: {str(e)}\\n\\nTraceback:\\n{detailed_error}\"\n",
    "    success = False\n",
    "finally:\n",
    "    if not final_status_message: # Fallback por si algo muy extraño ocurre\n",
    "        final_status_message = \"FAILURE - Raw to Bronze: Estado final desconocido o ejecución interrumpida.\"\n",
    "        success = False\n",
    "    \n",
    "    print(f\"INFO: Mensaje de salida final del notebook: {final_status_message}\")\n",
    "    if success:\n",
    "        dbutils.notebook.exit(final_status_message)\n",
    "    else:\n",
    "        # Cuando se usa dbutils.notebook.exit con un string que NO comienza con \"SUCCESS\", \n",
    "        # el job se marca como fallido y el string es el mensaje de error.\n",
    "        dbutils.notebook.exit(final_status_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7880a898-7951-48a2-a984-35a6dd6e0521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>0</td><td>2025-05-22T23:06:05Z</td><td>8703462587779018</td><td>fregod01@ucm.es</td><td>CREATE TABLE AS SELECT</td><td>Map(partitionBy -> [\"event_year\",\"event_month\"], clusterBy -> [], description -> null, isManaged -> false, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> false)</td><td>null</td><td>List(3766789225519571)</td><td>0521-060726-4p7u6oe0</td><td>null</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 4, numOutputRows -> 30, numOutputBytes -> 20087)</td><td>null</td><td>Databricks-Runtime/15.4.x-scala2.12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         "2025-05-22T23:06:05Z",
         "8703462587779018",
         "fregod01@ucm.es",
         "CREATE TABLE AS SELECT",
         {
          "clusterBy": "[]",
          "description": null,
          "isManaged": "false",
          "partitionBy": "[\"event_year\",\"event_month\"]",
          "properties": "{\"delta.enableDeletionVectors\":\"true\"}",
          "statsOnLoad": "false"
         },
         null,
         [
          "3766789225519571"
         ],
         "0521-060726-4p7u6oe0",
         null,
         "WriteSerializable",
         true,
         {
          "numFiles": "4",
          "numOutputBytes": "20087",
          "numOutputRows": "30"
         },
         null,
         "Databricks-Runtime/15.4.x-scala2.12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobRunId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELDA PARA LISTAR Y CONSULTAR BRONZE (DELTA LAKE)\n",
    "\n",
    "bronze_path_to_list = paths_for_listing.get(\"bronze_delta_path\")\n",
    "bronze_table_to_query = paths_for_listing.get(\"bronze_table_fullname\")\n",
    "\n",
    "# Reutilizar la función de listado recursivo de la celda anterior\n",
    "# def list_files_recursively_generic(path_to_list, path_label=\"BRONZE\"): ... (ya definida)\n",
    "\n",
    "if bronze_path_to_list:\n",
    "    print(f\"\\n--- Contenido de Archivos de la Capa BRONZE (Tabla Delta) ---\")\n",
    "    print(f\"Ruta física a listar: {bronze_path_to_list}\")\n",
    "    print(\"NOTA: Esperar estructura de tabla Delta con _delta_log y archivos de datos Parquet, particionada.\")\n",
    "    try:\n",
    "        top_level_files_bronze = dbutils.fs.ls(bronze_path_to_list)\n",
    "        if not top_level_files_bronze:\n",
    "            print(\"El directorio de la tabla bronze está vacío o no existe.\")\n",
    "        else:\n",
    "            list_files_recursively_generic(bronze_path_to_list, \"BRONZE\")\n",
    "    except Exception as e:\n",
    "        if \"java.io.FileNotFoundException\" in str(e):\n",
    "            print(f\"El directorio de la tabla bronze no existe: {bronze_path_to_list}\")\n",
    "        else:\n",
    "            print(f\"Error listando el directorio de la tabla bronze: {e}\")\n",
    "else:\n",
    "    print(\"ERROR: La ruta física de Bronze (Delta Path) no está definida para listar.\")\n",
    "\n",
    "\n",
    "if bronze_table_to_query:\n",
    "    print(f\"\\n--- Consultando la Tabla Delta '{bronze_table_to_query}' ---\")\n",
    "    try:\n",
    "        # Asegurar que la base de datos exista antes de intentar consultar la tabla\n",
    "        db_name_only = bronze_table_to_query.split('.')[0] if '.' in bronze_table_to_query else \"default\"\n",
    "        if db_name_only != \"default\": # \"default\" siempre existe\n",
    "             spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name_only}\")\n",
    "        \n",
    "        print(f\"Verificando si la tabla {bronze_table_to_query} existe...\")\n",
    "        table_exists = spark._jsparkSession.catalog().tableExists(bronze_table_to_query) # Para nombres de tabla completos\n",
    "\n",
    "        if table_exists:\n",
    "            print(f\"Tabla {bronze_table_to_query} existe. Mostrando esquema y datos de ejemplo:\")\n",
    "            df_bronze = spark.table(bronze_table_to_query)\n",
    "            \n",
    "            print(\"\\nEsquema de la tabla BRONZE:\")\n",
    "            df_bronze.printSchema()\n",
    "            \n",
    "            print(f\"\\nNúmero total de registros en BRONZE: {df_bronze.count()}\")\n",
    "            \n",
    "            print(\"\\nMostrando algunas filas de la capa BRONZE:\")\n",
    "            columnas_a_mostrar = [\"order_id\", \"order_date\", \"promo_code\", \"event_year\", \"event_month\"] # Ejemplo\n",
    "            cols_existentes = [c for c in columnas_a_mostrar if c in df_bronze.columns]\n",
    "            if cols_existentes:\n",
    "                df_bronze.select(*cols_existentes).show(10, truncate=False)\n",
    "            else:\n",
    "                df_bronze.show(10, truncate=False)\n",
    "            \n",
    "            # Historial de la tabla Delta\n",
    "            print(f\"\\n--- Historial de la Tabla Delta '{bronze_table_to_query}' ---\")\n",
    "            display(spark.sql(f\"DESCRIBE HISTORY {bronze_table_to_query}\"))\n",
    "\n",
    "        else:\n",
    "            print(f\"ERROR: La tabla Delta '{bronze_table_to_query}' no existe en el metastore.\")\n",
    "            print(f\"       Verifica si el proceso 'main_promote_raw_to_bronze.py' se ejecutó correctamente y registró la tabla.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al consultar la tabla Delta '{bronze_table_to_query}': {e}\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"ERROR: El nombre completo de la tabla Bronze no está definido para consulta.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7155413793922938,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "prueba_farmia_engine",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}