{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8333ec9b-9bfb-4cae-bb7b-d655b8764496",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Usando landing_path global: abfss://landing@masterfrl001sta.dfs.core.windows.net\nINFO: Los archivos CSV para evolución de esquema se generarán bajo: abfss://landing@masterfrl001sta.dfs.core.windows.net/sales_online\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import uuid\n",
    "import datetime # Asegúrate de que datetime esté importado\n",
    "from datetime import timedelta\n",
    "from pyspark.sql import SparkSession, DataFrame # DataFrame para type hinting\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "\n",
    "# Celda 1 del Notebook\n",
    "storage_account_name = spark.conf.get(\"adls.account.name\") # O tu nombre de cuenta fijo\n",
    "landing_container_name = \"landing\" \n",
    "landing_path = f\"abfss://{landing_container_name}@{storage_account_name}.dfs.core.windows.net\"\n",
    "print(f\"INFO: Usando landing_path global: {landing_path}\")\n",
    "\n",
    "output_target_folder_in_landing = \"sales_online\" # Esta será la carpeta {datasource}/{dataset}\n",
    "\n",
    "print(f\"INFO: Los archivos CSV para evolución de esquema se generarán bajo: {landing_path}/{output_target_folder_in_landing}\")\n",
    "\n",
    "\n",
    "# --- 2. Datos de Muestra Base ---\n",
    "PRODUCT_IDS = ['PROD001', 'PROD002', 'PROD003', 'PROD004', 'PROD005']\n",
    "CUSTOMER_IDS = ['CUST001', 'CUST002', 'CUST003', 'CUST004', 'CUST005']\n",
    "PROMO_CODES = [None, \"SAVE10\", \"NEW20\", None, \"SUMMERDEAL\", \"FREEBIE\"]\n",
    "\n",
    "# --- 3. Funciones Auxiliares ---\n",
    "def _get_timestamp_filename_suffix_nodate(): # Solo H M S f\n",
    "    return datetime.datetime.utcnow().strftime(\"%H%M%S%f\")[:-3]\n",
    "\n",
    "def _generate_base_sales_row_data_for_spark(include_promo_code: bool):\n",
    "    order_datetime_obj = datetime.datetime.utcnow() - timedelta(seconds=random.randint(0, 3600*2))\n",
    "    row = {\n",
    "        \"order_id\": str(uuid.uuid4()), \"product_id\": random.choice(PRODUCT_IDS),\n",
    "        \"quantity\": random.randint(1, 5), \"price\": round(random.uniform(10.0, 200.0), 2),\n",
    "        \"customer_id\": random.choice(CUSTOMER_IDS), \"order_date\": order_datetime_obj\n",
    "    }\n",
    "    if include_promo_code:\n",
    "        row[\"promo_code\"] = random.choice(PROMO_CODES)\n",
    "    return row\n",
    "\n",
    "# --- 4. Función Principal de Generación de Archivo Único (usando Spark y lógica de rutas similar a tu ejemplo) ---\n",
    "def generate_single_csv_file_via_spark(\n",
    "    spark_session: SparkSession,\n",
    "    dbutils_object, # Pasar dbutils\n",
    "    base_landing_path: str, # ej. abfss://landing@<cuenta>.dfs.core.windows.net\n",
    "    datasource_name: str, # ej. \"farmia\" o \"ventas\"\n",
    "    dataset_name: str,    # ej. \"sales_online\" o \"pedidos\"\n",
    "    file_name_prefix: str, # ej. \"sales_orig_schema\"\n",
    "    schema_for_df: StructType, \n",
    "    num_rows: int, \n",
    "    include_promo_code: bool,\n",
    "    output_format: str = \"csv\" # Formato de salida\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera un DataFrame, lo guarda temporalmente y luego lo mueve a una ruta final estructurada\n",
    "    con fecha (YYYY/MM/DD) y nombre de archivo con timestamp.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Crear DataFrame\n",
    "    print(f\"  Generando {num_rows} filas de datos para: {datasource_name}/{dataset_name} (promo: {include_promo_code})\")\n",
    "    data_to_write = [_generate_base_sales_row_data_for_spark(include_promo_code) for _ in range(num_rows)]\n",
    "    try:\n",
    "        df = spark_session.createDataFrame(data_to_write, schema=schema_for_df)\n",
    "    except Exception as e_df:\n",
    "        print(f\"  ERROR creando DataFrame: {e_df}\")\n",
    "        return None\n",
    "\n",
    "    # 2. Escribir a una ruta temporal (global bajo landing_path o específica del dataset)\n",
    "    # Usaremos una ruta temporal específica del dataset para evitar colisiones y facilitar limpieza.\n",
    "    dataset_base_path_in_landing = f\"{base_landing_path.rstrip('/')}/{dataset_name}\"\n",
    "    tmp_uuid = str(uuid.uuid4().hex[:6])\n",
    "    tmp_path_adls = f\"{dataset_base_path_in_landing}/_tmp_spark_write_{file_name_prefix}_{tmp_uuid}/\" \n",
    "    csv_writer_timestamp_format = \"yyyy-MM-dd HH:mm:ss\" \n",
    "    print(f\"    Escribiendo DataFrame temporalmente a: {tmp_path_adls}\")\n",
    "    try:\n",
    "        df.coalesce(1).write.format(output_format)\\\n",
    "            .option(\"timestampFormat\", csv_writer_timestamp_format) \\\n",
    "            .option(\"header\", \"true\").mode(\"overwrite\").save(tmp_path_adls)\n",
    "    except Exception as e_write:\n",
    "        print(f\"    ERROR al escribir temporalmente a {tmp_path_adls}: {e_write}\")\n",
    "        try: dbutils_object.fs.rm(tmp_path_adls, True)\n",
    "        except: pass\n",
    "        return None\n",
    "\n",
    "    # 3. Preparar ruta final y nombre de archivo (SIN subcarpetas de fecha)\n",
    "    now = datetime.datetime.utcnow()\n",
    "    timestamp_for_filename = now.strftime(\"%Y%m%d%H%M%S%f\")[:-3] # Sigue siendo útil para nombres de archivo únicos\n",
    "    \n",
    "    # Directorio de destino final para el archivo: landing_path/datasource_name/dataset_name/\n",
    "    final_target_directory = dataset_base_path_in_landing # Ya no se añade /YYYY/MM/DD\n",
    "    \n",
    "    final_file_name_part = f\"{file_name_prefix}_{timestamp_for_filename}.{output_format}\"\n",
    "    final_file_destination_path = f\"{final_target_directory.rstrip('/')}/{final_file_name_part}\"\n",
    "\n",
    "    # Crear el directorio de destino final si no existe\n",
    "    dbutils_object.fs.mkdirs(final_target_directory)\n",
    "\n",
    "    # 4. Mover el archivo desde la carpeta temporal a la final\n",
    "    moved_successfully = False\n",
    "    try:\n",
    "        files_in_tmp = dbutils_object.fs.ls(tmp_path_adls)\n",
    "        part_file_to_move = None\n",
    "        for file_info in files_in_tmp:\n",
    "            if file_info.name.startswith(\"part-\") and (output_format == \"csv\" or file_info.name.endswith(f\".{output_format}\")):\n",
    "                part_file_to_move = file_info.path\n",
    "                break \n",
    "        \n",
    "        if part_file_to_move:\n",
    "            print(f\"    Moviendo archivo: {part_file_to_move} -> {final_file_destination_path}\")\n",
    "            dbutils_object.fs.mv(part_file_to_move, final_file_destination_path)\n",
    "            print(f\"  Archivo CSV guardado exitosamente en: {final_file_destination_path}\")\n",
    "            moved_successfully = True\n",
    "        elif files_in_tmp: # Hubo archivos en tmp pero no el part-file esperado\n",
    "            print(f\"  ADVERTENCIA: No se encontró el archivo 'part-*.{output_format}' esperado en {tmp_path_adls} para mover. Contenido: {[f.name for f in files_in_tmp]}\")\n",
    "        else: # tmp_path_adls estaba vacío\n",
    "             print(f\"  ADVERTENCIA: El directorio temporal {tmp_path_adls} estaba vacío después de la escritura de Spark.\")\n",
    "\n",
    "    except Exception as e_mv:\n",
    "        print(f\"  ERROR al mover el archivo desde '{tmp_path_adls}'. Error: {e_mv}\")\n",
    "    finally:\n",
    "        # 5. Limpiar la carpeta temporal\n",
    "        print(f\"    Limpiando directorio temporal: {tmp_path_adls}\")\n",
    "        try:\n",
    "            dbutils_object.fs.rm(tmp_path_adls, True)\n",
    "        except Exception as e_rm:\n",
    "            print(f\"    ADVERTENCIA: No se pudo eliminar el directorio temporal '{tmp_path_adls}'. Error: {e_rm}\")\n",
    "\n",
    "    return final_file_destination_path if moved_successfully else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb62bca3-34b1-4a0f-b9c8-ef782d692740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " # Esquema original\n",
    "schema_original = StructType([\n",
    "    StructField(\"order_id\", StringType(), True), StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True), StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True), StructField(\"order_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Esquema evolucionado\n",
    "schema_evolved = StructType(schema_original.fields + [StructField(\"promo_code\", StringType(), True)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1913f7b-6f55-437e-aef8-0a0f0e02720f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Para generar archivos de sales_online con el esquema original\n",
    "generate_single_csv_file_via_spark(\n",
    "    spark_session=spark,\n",
    "    dbutils_object=dbutils,\n",
    "    base_landing_path=landing_path, # La raíz donde se creará la carpeta \"sales_online\"\n",
    "    file_name_prefix=\"sales_online\",\n",
    "    schema_for_df=schema_original,\n",
    "    output_format=\"csv\",\n",
    "    datasource_name=\"sales_online\",\n",
    "    dataset_name=\"sales_online\",\n",
    "    num_rows=random.randint(5, 10),\n",
    "    include_promo_code=False\n",
    ")\n",
    "dbutils.notebook.exit(\"Generación de CSVs para evolución de esquema con Spark completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82c87811-e06a-4e4b-9b27-fcd7de48fa34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Para generar archivos de sales_online con el esquema modificar con columna extra cupon\n",
    "generate_single_csv_file_via_spark(\n",
    "    spark_session=spark,\n",
    "    dbutils_object=dbutils,\n",
    "    base_landing_path=landing_path, # La raíz donde se creará la carpeta \"sales_online\"\n",
    "    file_name_prefix=\"sales_online\",\n",
    "    schema_for_df=schema_evolved,\n",
    "    output_format=\"csv\",\n",
    "    datasource_name=\"sales_online\",\n",
    "    dataset_name=\"sales_online\",\n",
    "    num_rows=random.randint(5, 10),\n",
    "    include_promo_code=True\n",
    ")\n",
    "dbutils.notebook.exit(\"Generación de CSVs para evolución de esquema con Spark completada.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "generar_sample_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}